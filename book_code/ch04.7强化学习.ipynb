{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 强化学习Q-Learning求解最佳路径\n",
    "\n",
    "强化学习中的状态、决策、状态转移、奖励等可以用马尔可夫决策过程（Markov decision processes，简称MDP）来刻画。MDP可以用一个五元组（S,A,P_s a,γ,R）表示。\n",
    "\n",
    "```\n",
    "S是所有状态的集合。例如机器人寻宝中机器人的位置。\n",
    "\tA是所有动作的集合。例如机器人的（向东、向南、向西、向北）的动作。\n",
    "\tP_sa (s')是状态转移概率。给出了对于任何一个状态s和该状态下的动作a，将转移到的下一个状态s'的概率。\n",
    "\tγ∈[0,1] 是“折扣因子”。表示未来奖励对于当前动作的作用有多大。\n",
    "\tR是一个S×A→R的奖励函数，即在状态s下采取动作a所得到的直接奖励。\n",
    "```\n",
    "![](imgs/rl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7.2 Q-Learning\n",
    "\n",
    "Q-Learning算法的过程如下：\n",
    "```\n",
    "初始化Q(s,a)=0\n",
    "多次(如200次)episode:\n",
    "对每个episode，选择一个出发状态s，执行下面的循环: \n",
    "                     ϵ贪婪法选择一个动作a\n",
    "得到环境反馈的(r,s')\n",
    "如果s'不是结束状态，则更新Q(s,a)，s = s'。 否则，这次episode结束\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7.3 Q-Learning的Python实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Q-table:\n",
      "\n",
      "[{'s': -0.814697981114816, 'e': 0.0}, {'w': 0.0, 'e': 0.44818549935293456}, {'w': 0.11081761095206341, 's': 0.9994360791266039, 'e': 0.21807779531419424}, {'w': 0.8965111343404737, 'e': 0.4516150503836326}, {'w': 0.7967613112390031, 's': -0.7458134171671}]\n"
     ]
    }
   ],
   "source": [
    "# q_Table\n",
    "def build_Q_table(state_actions = None):\n",
    "    Q = {}\n",
    "    if state_actions == None:\n",
    "        Q =  [ {'s':0,'e':0},{'w':0,'e':0},{'w':0,'s':0,'e':0},{'w':0,'e':0},{'w':0,'s':0}]\n",
    "    else:\n",
    "        for actions in state_actions:\n",
    "            for action in actions:\n",
    "                action.append(0)\n",
    "    return Q\n",
    "\n",
    "\n",
    "#初始化游戏环境：状态转移及奖励、终止状态\n",
    "def build_T_table(transit_table = None,terminal_states = None):\n",
    "    if transit_table == None:\n",
    "        transit_table =  {(0,'s'):(5,-1),(0,'e'):(1,0),(1,'w'):(0,0),(1,'e'):(2,0),(2,'s'):(6,1),(2,'w'):(1,0),(2,'e'):(3,0),\n",
    "              (3,'w'):(2,0),(3,'e'):(4,0),(4,'w'):(3,0),(4,'s'):(7,-1)}\n",
    "    if terminal_states==None:\n",
    "        terminal_states = {5,6,7}\n",
    "    return transit_table,terminal_states\n",
    "\n",
    "\n",
    "def choose_action(state, Q,EPSILON=0.1):\n",
    "    action_values = Q[state]\n",
    "    if random.random() < EPSILON :        \n",
    "        action_name = random.choice(list(action_values))\n",
    "    else:\n",
    "        max_elem = max(action_values,key=action_values.get) \n",
    "        action_name  = max_elem[0]\n",
    "    return action_name\n",
    "\n",
    "def get_env_feedback(state, action,transit_table,terminal_states):\n",
    "    next_state,reward = transit_table[(state,action)]\n",
    "    is_terminal = next_state in terminal_states\n",
    "    return next_state,reward,is_terminal\n",
    "\n",
    "import random\n",
    "def random_start_state(size,terminal_states):\n",
    "    while True:\n",
    "        s = random.randint(0,size-1)\n",
    "        if s not in terminal_states:\n",
    "            return s\n",
    "\n",
    "def Q_Learning(Q,transit_table ,terminal_states,\n",
    "               MAX_EPISODES = 15,EPSILON = 0.2,ALPHA = 0.1,GAMMA = 0.9): \n",
    "        \n",
    "    for episode in range(MAX_EPISODES):  # 循环的回合数 \n",
    "        step_counter = 0\n",
    "        s = random_start_state(len(Q),terminal_states)\n",
    "        s  = 4\n",
    "        is_terminated = False\n",
    "        while not is_terminated:  # 循环直到一局游戏结束\n",
    "            action = choose_action(s, Q,EPSILON)  # 根据状态选择动作\n",
    "# 获取环境的反馈       \n",
    "            s_next, R, is_terminated = get_env_feedback(s, action,transit_table,terminal_states)                 \n",
    "            q_predict = Q[s][action]\n",
    "            if not is_terminated:     # 如果没有结束就更新q_target值\n",
    "                action_values = Q[s_next]\n",
    "                #max_action = max(action_values,key=action_values.get)                \n",
    "                q_target = R + GAMMA * action_values[max(action_values,key=action_values.get)] \n",
    "            else:                    # S_是结束状态，\n",
    "                q_target = R    \n",
    "          \n",
    "            Q[s][action] += ALPHA * (q_target - q_predict)  #更新Q值\n",
    "            s = s_next  # 进入下一状态            \n",
    "    return Q\n",
    " \n",
    "Q = build_Q_table()\n",
    "transit_table,terminal_states = build_T_table()\n",
    "Q = Q_Learning(Q,transit_table,terminal_states,100)\n",
    "print('\\r\\nQ-table:\\n')\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path:\n",
      " [0, 1, 2, 6]\n"
     ]
    }
   ],
   "source": [
    "def shortest_path(state,Q,transit_table,terminals):\n",
    "    path = []\n",
    "    count=0\n",
    "    while state not in terminals:\n",
    "        path.append(state)      \n",
    "        action_values = Q[state]\n",
    "        action = max(action_values,key=action_values.get)\n",
    "        s_next,reward = transit_table[(state,action)]\n",
    "        state = s_next      \n",
    "    path.append(state)\n",
    "    return path\n",
    "s = 0\n",
    "path = shortest_path(s,Q,transit_table,terminal_states)\n",
    "print(\"path:\\n\",path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个Q-Learning实现不同于网上针对特定问题的特定实现，除2个辅助函数buildTtable()和buildTtable是针对寻宝问题的，其他的函数都可用于其他的类似问题。如网上的“无痛Q-Learning”的房间问题、迷宫问题、QL玩FlappyBird游戏等。即这是一个通用性的Q-Learning程序实现。篇幅有限，这里仅仅以迷宫问题为例，说明程序的通用性。\n",
    "针对迷宫问题，只需要初始化Q表和状态转移表（包括终止状态集合）就可以了，这里用一个辅助函数initgamemaze()从一个迷宫的二维数组初始化程序的Q表和状态转移表（包括终止状态集合）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Q-table:\n",
      "\n",
      "[{'D': 0.4282659404761937, 'R': 0.5904891698542925},\n",
      " {'D': -0.9866972053527088, 'L': 0.4480106309112527, 'R': 0.6560997016470077},\n",
      " {'D': 0.7289999080757529, 'L': 0.46085007742034534, 'R': 0.5929119297418985},\n",
      " {'D': 0.7936945368133071, 'L': 0.3069859007842442},\n",
      " {'D': 0.23817333884122693, 'R': -0.9999999965733934, 'U': 0.5314392197210498},\n",
      " {'D': 0, 'L': 0, 'R': 0, 'U': 0},\n",
      " {'D': -0.7712320754503901,\n",
      "  'L': -0.717570463519,\n",
      "  'R': 0.8099999814532228,\n",
      "  'U': 0.5540872987310341},\n",
      " {'D': 0.8999999980985273, 'L': 0.5630843970503903, 'U': 0.5451212362124451},\n",
      " {'D': 0.0, 'R': -0.7712320754503901, 'U': 0.40621377698596123},\n",
      " {'D': 0, 'L': 0, 'R': 0, 'U': 0},\n",
      " {'D': 0, 'L': 0, 'R': 0, 'U': 0},\n",
      " {'D': 0.9999999996963022, 'L': -0.9282102012308148, 'U': 0.5271814388255629},\n",
      " {'R': 0.0, 'U': 0.0},\n",
      " {'L': 0.0, 'R': 0, 'U': -0.1},\n",
      " {'L': 0, 'R': 0, 'U': 0},\n",
      " {'L': 0, 'U': 0}]\n",
      "path:  [0, 1, 2, 6, 7, 11, 15]\n"
     ]
    }
   ],
   "source": [
    "def init_game_maze(maze):\n",
    "    m = len(maze)\n",
    "    n = len(maze[0])\n",
    "    s = 0\n",
    "    Q = []\n",
    "    T = dict()\n",
    "    terminals = set()\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            Q.append(dict())\n",
    "            if i>=1:\n",
    "                s_ = s-n\n",
    "                Q[s]['U'] = 0\n",
    "                T[(s,'U')] = (s_,maze[i-1][j])   \n",
    "            if i<m-1:\n",
    "                s_ = s+n\n",
    "                Q[s]['D'] = 0\n",
    "                T[(s,'D')] = (s_,maze[i+1][j]) \n",
    "            if j>=1:\n",
    "                s_ = s-1\n",
    "                Q[s]['L'] = 0\n",
    "                T[(s,'L')] = (s_,maze[i][j-1])   \n",
    "            if j<m-1:\n",
    "                s_ = s+1\n",
    "                Q[s]['R'] = 0\n",
    "                T[(s,'R')] = (s_,maze[i][j+1]) \n",
    "            if maze[i][j]!=0:\n",
    "                terminals.add(s)\n",
    "            s+=1\n",
    "    return Q,T,terminals\n",
    "\n",
    "maze = [[0, 0, 0, 0],\n",
    "        [0, -1, 0, 0],\n",
    "        [0, -1, -1, 0],\n",
    "        [0, -0, 0, 1]]\n",
    "\n",
    "import pprint\n",
    "if __name__ == \"__main__\":\n",
    "    Q,T,terminals = init_game_maze(maze)\n",
    "    Q = Q_Learning(Q,T,terminals,500)\n",
    "    print('\\r\\nQ-table:\\n')\n",
    "    pprint.pprint(Q)  #print(Q)\n",
    "    s = 0\n",
    "    path = shortest_path(s,Q,T,terminals)\n",
    "    print(\"path: \",path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
